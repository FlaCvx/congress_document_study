{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraping images for all the items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "#Local path where you want the images to be saved. The image will be saved based in the item, and they will be saved as\n",
    "# \"page_0, page_1, ...\"\n",
    "destination_path = \"/home/fla/Desktop/Research_Assistantship/Congress_Documents_Scraping/1824to1837_DebatesAndProceedings/Item_\"\n",
    "\n",
    "\n",
    "#Name of the image saved in the webste. Lookup with \"Inspection element\"\n",
    "name_of_the_image = \"img src=\\\"/ll/llrd\" \n",
    "\n",
    "num_items = 29\n",
    "#The lenghts of each item need to be manually set\"\n",
    "lenghts = [507, 831, 717, 840, 739, 753, 485, 667, 647, 677,\n",
    "           737, 747, 717, 670, 645, 699, 695, 722, 697, 643,\n",
    "           651, 643, 643, 643, 599, 723, 705, 596, 641]\n",
    "\n",
    "appendix_start = [376, 819, 536, 784, -1,  652,  404, -1,  506, 430,\n",
    "                  714, -1,  516, -1,  318, -1,   -1,  -1,  346,  -1, \n",
    "                  206, -1,  670, -1,  438, 708 , 404, 588, 292]\n",
    "\n",
    "#Used to fill the dictionary automatically\n",
    "first_part_url=\"https://memory.loc.gov/cgi-bin/ampage?collId=llrd&fileName=\"\n",
    "second_part_url=\"/llrd\"\n",
    "third_part_url=\".db&recNum=\"\n",
    "\n",
    "\n",
    "vol = {}\n",
    "for i in range(1, num_items+1, 1):\n",
    "    vol[i] = []\n",
    "    if (i<10):\n",
    "        vol.get(i).append(first_part_url+\"00\"+str(i)+second_part_url+\"00\"+str(i)+third_part_url)\n",
    "    elif(i<100):\n",
    "        vol.get(i).append(first_part_url+\"0\"+str(i)+second_part_url+\"0\"+str(i)+third_part_url)\n",
    "    else:\n",
    "        vol.get(i).append(first_part_url+str(i)+second_part_url+str(i)+third_part_url)\n",
    "        \n",
    "    vol.get(i).append(name_of_the_image)\n",
    "    vol.get(i).append(lenghts[i-1])\n",
    "    vol.get(i).append(appendix_start[i-1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_item(url, pattern, start, end, destination_path ):\n",
    "    for count in range(start,end+1, 1):\n",
    "        new_url = url+str(count)\n",
    "        f = requests.get(new_url)\n",
    "        name = extract_name(f.text, (f.text).find(pattern)  )\n",
    "        complete_path = \"https://memory.loc.gov\"+str(name)\n",
    "        urllib.request.urlretrieve(complete_path, destination_path+\"/page_\"+str(count))\n",
    "    for count in range(0,num_max+1, 1):\n",
    "        im = Image.open(destination_path+\"/page_\"+str(count))\n",
    "        im.save( destination_path+\"/page_\"+str(count),'PNG')\n",
    "        \n",
    "        \n",
    "def extract_name(raw_line, start_index):\n",
    "    end = start_index\n",
    "    while (raw_line[end:end+3]!=\"gif\"):\n",
    "        end += 1\n",
    "    end +=3\n",
    "    return raw_line[start_index+9:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in range(23,len(vol)+1,1):\n",
    "#for item in range(1,2,1):\n",
    "    if(vol.get(item)[3]!=-1):\n",
    "        if not os.path.exists(destination_path+str(item)):\n",
    "            os.makedirs(destination_path+str(item))\n",
    "#           scrape_item(url, pattern, start, end, destination_path ):\n",
    "            scrape_item(vol.get(item)[0], \n",
    "                         vol.get(item)[1], \n",
    "                         0,\n",
    "                         vol.get(item)[3], #Start of the appendix\n",
    "                         destination_path+str(item))\n",
    "        path=destination_path+str(item)+'/Appendix'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "#           scrape_item(url, pattern, start, end, destination_path ):\n",
    "            scrape_item(vol.get(item)[0], \n",
    "                         vol.get(item)[1], \n",
    "                         vol.get(item)[3],\n",
    "                         vol.get(item)[2], \n",
    "                         path)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #There is no appendix.\n",
    "        if not os.path.exists(destination_path+str(item)):\n",
    "            os.makedirs(destination_path+str(item))\n",
    "            scrape_item(vol.get(item)[0], \n",
    "                         vol.get(item)[1], \n",
    "                         0,\n",
    "                         vol.get(item)[2], \n",
    "                         destination_path+str(item))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
